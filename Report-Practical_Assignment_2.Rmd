---
title: "Practical Assignment 2 - Web and Text Mining"
author: "Jo√£o Pires, Vanessa Silva"
date: "30 de Abril de 2017"
output: html_document
---
<br /> 

This report follows the analysis of [IMDb](http://www.imdb.com) movies and their reviews, is the retrieval of relevant information on movies and the analysis of the reviews (and the corresponding scores) assigned by users.
For this we will use our knowledge regarding web and text mining.

*Web Mining* consists of extracting information from the content of the pages, their links, and users' browsing logs, using data mining tools. 
Thus, we can divide Web Mining into three variants: Web Content Mining; Web Structure Mining; And Web Usage Mining.

*Text Mining* consists of extracting useful information from a collection of documents.
Involves basic pre-processing/text mining operations, such as identification/extraction of representative features, and identification of complex patterns as, e.g. relationships between previously identified concepts.
Text Mining exploits techniques/methodologies from data mining, machine learning, information retrieval, and corpus-based computational linguistics, where corpus is a collection of documents.


**The Goal**

To use web and text mining to extract and study important informations in the movies and their reviews from the IMDb website.


##Necessary Packages

```{r, eval=FALSE, message=FALSE, warning=FALSE, results='hide'}
# Install the necessary packages
install.packages("rvest")
install.packages("tm")
install.packages("wordcloud")
install.packages("SnowballC")
install.packages("qdap")
install.packages("e1071")
install.packages("earth")
install.packages("DMwR")
install.packages("performanceEstimation")
```


```{r, message=FALSE, warning=FALSE}
# Load the necessary packages
library(rvest)
library(tm)
library(wordcloud)
library(qdap)
library(e1071)
library(earth)
library(DMwR)
library(performanceEstimation) 
```


##Tasks

Using the information available in the IMDb site we will accomplish a series of tasks below.


###Find basic information

First we will find basic information as web page, diretor, cast, etc. of a movie based on a query string of the title.

In order to do this, we present a function (``listMovies``) that the user can call with a search string and a limit number (parameter we want to add) from the list of movies that will be returned corresponding to that string, since the string can be incomplete and therefore potentially match more than one movie in the site's database. The list returned by the function is ordered decreasingly by the order of confidence, where we stipulate to be the rating of the films.

``listMovies`` is aided by a set of other functions presented (``Searchform``, ``listMoviesAll`` and ``webPages``), which, respectively, help to: submit the form in `imdb.com` with the search string, find the list of all  movies matching the string that the site presents, and get the links from the web pages of all those movies.


```{r}
# Store web url
url_IMBD <- "http://www.imdb.com/"

# Submit the form on imdb.com for a movie description
Searchform <- function(session, query, type) {
  form <- html_form(session)[[1]]
  form <- set_values(form, q = query, s = type)
 
  return(submit_form(session, form))
}

# Returns web pages link
webPages <- function(listResults) {
  
  results <- listResults[[2]]
  
  # Get web page link
  movieLink <- (results %>% html_nodes(".result_text > a") %>% html_attr("href"))
  movieLink <- paste(url_IMBD, substring(movieLink, 2), sep='')
  
  return(movieLink)
  
}

# Returns a list of all movies matching the search string in the site
listMoviesAll <- function(query, type) {
  #Simulate a session in an html browser
  sessionMovies <- html_session(url_IMBD)
  
  results <- Searchform(sessionMovies, query, type)
  
  return(list(results %>% html_nodes(".result_text") %>% html_text(), results))
}

# Returns a list of the movies matching the search string, by decreasing order of confidence
listMovies <- function(query, maxNum) {
  type <- "tt"
  
  r <- listMoviesAll(query, type)
  
  TMlist <- r[[1]]
  
  movieLink <- webPages(r)
  
  # get rankings
  movieRanking <- lapply(movieLink, . %>% read_html() %>% html_nodes("strong span") %>% html_text())
  movieRanking <- as.array(movieRanking)
  
  # get data frame with movies and their ranking
  ResulList <- data.frame(as.array(TMlist),movieRanking)
  names(ResulList) <- c("Movies", "Ranking")
  ResulList$Ranking <- as.double(ResulList$Ranking)
  
  return(as.list((head(ResulList[order(ResulList$Ranking, decreasing=TRUE), ], maxNum))$Movies))
}
```

We can now use this function ``listMovies`` with the Lego Movie.

```{r}
Query <- "Lego"       #search string
NumMovies <- 10       #extra parameter that controls the maximum number of retrieved movies
```

```{r, eval=FALSE}
(ResultListMovies <- listMovies(Query, NumMovies))
```


```{r, message=FALSE, warning=FALSE, include=FALSE}
load("ResultListMovies.Rdata")
```


After getting the movie list we will finally get the basic information for each movie. For this we need the links of the web page of each film. Since we change the order as the list is presented and we limit the size of the list, we need to do some manipulation to get the links correctly, since the ``webPages`` function returns the links of all the movies in the order they are presented on the site.

```{r}
# Get link from web pages of n movies

FullList <- listMoviesAll(Query, "tt")
results <- FullList[2][[1]]
movieLink <- webPages(list(results %>% html_nodes(".result_text") %>% html_text(), results))

links <- matrix(nrow=NumMovies,ncol=2)
idx <- 1

for(i in 1:length(movieLink))
  for(j in 1:length(ResultListMovies))
    if (ResultListMovies[j][[1]] == FullList[1][[1]][i]){
      links[idx, 1] <- movieLink[i]
      links[idx,2] <- j
      idx <- idx+1
    }

movieLink <- matrix(ncol = NumMovies)

for(i in 1:10) {
  idx <- as.integer(links[i,2])
  movieLink[idx] <- links[i,1]
}

(movieLink <- as.character(movieLink))
```

Having the link to the movie's webpage and the CSS selectors we want, we can finally get the basic information for each movie. We will present the following basic information: webpage, original title, genre, rating, description, storyline, cast, date, runtime and metascore, on the form of list for each movie, each film being an element of a list as well.

We can see that some movie sites do not contain some of the requested information, obtaining the following result *character(0)*.

```{r, message=FALSE, warning=FALSE}
listBasInf <- list()

for(i in 1:NumMovies) {
  l <- list()
  j <- 1
  
  ##web page
  l[[j]] <- list(WebPage = (movieLink[i] %>% read_html()))
  j <- j+1
  ##original title
  l[[j]] <- list(OriginalTitle = (movieLink[i] %>% read_html() %>% html_nodes(".originalTitle") %>% html_text()))
  j <- j+1
  ##genre
  l[[j]] <- list(Genre = (movieLink[i] %>% read_html() %>% html_nodes(".subtext .itemprop") %>% html_text()))
  j <- j+1
  ##rating
  l[[j]] <- list(Ratink = (movieLink[i] %>% read_html() %>% html_nodes("strong span") %>% html_text()))
  j <- j+1
  ##description
  l[[j]] <- list(Description = (movieLink[i] %>% read_html() %>% html_nodes(".summary_text") %>% html_text()))
  j <- j+1
  ##storyline
  l[[j]] <- list(Storyline = (movieLink[i] %>% read_html() %>% html_nodes("#titleStoryLine p") %>% html_text()))
  j <- j+1
  ##cast
  l[[j]] <- list(Cast = (movieLink[i] %>% read_html() %>% html_nodes("#titleCast .itemprop span") %>% html_text()))
  j <- j+1
  ##date
  l[[j]] <- list(Date = (movieLink[i] %>% read_html() %>% html_nodes(".subtext a~ .ghost+ a") %>% html_text()))
  j <- j+1
  ##runtime
  l[[j]] <- list(Runtime = (movieLink[i] %>% read_html() %>% html_nodes("#titleDetails time") %>% html_text())) 
  j <- j+1
  ##metascore
  l[[j]] <- list(Metascore = movieLink[i] %>% read_html() %>% html_nodes(".score_favorable span") %>% html_text())
  
  listBasInf[[i]] <- list(l)
}

listBasInf[[1]]
```


###Obtain the information on all reviews from a movie

We now present a set of functions to perform the task of finding all reviews for a movie, as well as the rating reviews given by the review writer.

One can read, in the page [User Reviews Guidelines](http://www.imdb.com/help/show_leaf?commentsguidelines) the following: "The minimum length for reviews is 5 lines of text."

With this we can see that the text field is mandatory, but the rating field is not. With that in mind, our function will return `r NA` whenever the user did not provide any rating. The rating provided is then an integer between 1 and 10, the scale given by IMDb for ratings.

```{r}
# Function to return the comments
commentOfReview <- function (linkOfMovie) {
  movie <- linkOfMovie %>% read_html()
  
  # Number of comments
  n_reviews <- as.integer(strsplit((movie %>% html_nodes(".user-comments a"))[4] %>% html_text(), " ")[[1]][3])
  
  # Link of the page with all reviews
  linkOfMovie <- gsub("\\?ref_=fn_tt_tt_[0-9]*", "", linkOfMovie)
  reviews_page_link <- paste(linkOfMovie, "reviews?count=", n_reviews, "&start=0", sep = "")
  
  comments <- reviews_page_link %>% read_html() %>% html_nodes("#pagecontent") %>% html_nodes("div+ p") %>% html_text()
  
  return(comments)
}

# Function to return rating of a review
ratingOfReview <- function(linkOfMovie) {
  movie <- linkOfMovie %>% read_html()
  
  # Number of comments
  n_reviews <- as.integer(strsplit((movie %>% html_nodes(".user-comments a"))[4] %>% html_text(), " ")[[1]][3])
  
  # Link of the page with all reviews
  linkOfMovie <- gsub("\\?ref_=fn_tt_tt_[0-9]*", "", linkOfMovie)
  reviews_page_link <- paste(linkOfMovie, "reviews?count=", n_reviews, "&start=0", sep = "")
  
  reviews_images <- reviews_page_link %>% read_html() %>% html_nodes("#tn15content") %>% html_nodes("div img")
  
  ans <- NULL
  cnt <- 1
  for(i in 1:length(reviews_images)) {
    if(!is.na(reviews_images[i] %>% html_attr("class")) && reviews_images[i] %>% html_attr("class") == "avatar") {
      if(i == length(reviews_images)) {
        ans[cnt] <- NA
        cnt <- cnt + 1
      }
      else {
        if(!is.na(reviews_images[i + 1] %>% html_attr("class")) && reviews_images[i + 1] %>% html_attr("class") == "avatar") {
          ans[cnt] <- NA
          cnt <- cnt + 1
        }
      }
    }
    else {
      ans[cnt] <- as.integer(substr(reviews_images[i] %>% html_attr("alt"), 1, 1))
      cnt <- cnt + 1
    }
  }
  
  return(ans)
}

# Function to return the comments and stars given in review
fullReviews <- function(linkOfMovie) {
  ans <- data.frame(comment = commentOfReview(linkOfMovie), rating = ratingOfReview(linkOfMovie), stringsAsFactors = FALSE)
  
  return(ans)
}
```

We can try now this with the Lego Movie, as an example:

```{r}
# Lego movie:
lego_movie_link <- "http://www.imdb.com/title/tt1490017/"
dat <- fullReviews(lego_movie_link)
head(dat)
```

###Model to predict the grade

The goal now is to build a model that can predict the grade of the review based on the text of such review.

####Build a data set for learning

For this, we will construct a data set (``dataSet``) suitable for learning of this model(s).

First, let's build a Corpus (a set of documents) with the reviews of the movie (we will continue with the previous example of the film "Lego") so that we can perform the preprocessing tasks (Clean-ups).

There are many ways of importing documents into a **tm** Corpus object. In this case we will import documents whitch are stored as a vector of strings (one per text document). This is the goal of the ``VectorSource()`` call - to tell the function VCorpus that the Corpus is to be read from a series of reviews in a vector of strings (previously created data frame column). 

```{r, message=FALSE, warning=FALSE}
(corp <- VCorpus(VectorSource(dat$comment)))                #Create a corpus using the reviews
```

For the preprocessing task let's use the ``tm_map()`` function of package **tm**, this function is one of the most important because it allows us to apply transformations to the texts forming a Corpus.
Transformations basically consist in applying a function to each of the texts forming a Corpus, removing information that carries no information for our goal.

Let's then remove white spaces, put everything in lowercase, remove punctuation characters and remove the numbers in the documents.
Let's also create a representation for each document using a bag of words approach. Some of the words on any text are too common to help in our goal, an example are stop words that occur on any language, which must be removed. Most words have many variants (e.g. ‚Äúbring‚Äù and ‚Äúbringing‚Äù), presenting the same concept and thus it makes sense to use a single descriptor for each of these concepts. So we should carry out the task of word stemming that picks the ‚Äúroot‚Äù of each word.

```{r}
#Preprocessing the corpus: Clean-ups
corp <-  tm_map(corp, stripWhitespace)                      # Strip extra whitespace
corp <-  tm_map(corp, content_transformer(tolower))         # Turn everything to lowercase
corp <-  tm_map(corp, removePunctuation)                    # Remove punctuation
corp <-  tm_map(corp, removeNumbers)                        # Remove numbers
corp1 <- tm_map(corp, removeWords, stopwords("english"))    # Remove stopwords
corp1 <- tm_map(corp1, stemDocument)                        # Stemming the words (keeping only the "root" of each word)
```

Now let's create document term matrices using the ``DocumentTermMatrix()`` function. These are matrices that contain one row per document and where the columns are terms (typically words) appearing in all corpus. By default the matrix is filled with term frequencies.

```{r}
(dtm <- DocumentTermMatrix(corp1))
inspect(dtm[, 10:11])
```

We can see that each document is still represented by many terms. We may carry out a further cleaning of some terms with the help of the function ``removeSparseTerms()`` that removes the terms that are too sparse.

```{r}
(dtm <- removeSparseTerms(dtm, 0.8))                        # Remove sparse terms

#Data set for learning
dataSet <- cbind(data.frame(as.matrix(dtm), class = dat$rating))
head(dataSet)
```

####Prediction models and Conclusions

Sometimes we have some tag associated with each document of a given Corpus. When that the value of these tags may depends on the content of the documents, we have what is called a **Classification Problem**.
These problems consist of trying to approximate an unknown function ``Y=f(X_1,X_2,...,X_p)``, where ``Y`` is the target variable (in our case is the number of stars given by the reviewer), and ``X_1,X_2,...,X_p`` are descriptor variables (in our case properties of the reviews).
The approximation of this unknown function is obtained based on examples where we know both the values of the descriptor variables (series of documents obtained of the reviews) and the values of the target variable (number of stars) - known as a **training set**.

Using the previous data set we will try a few prediction models and draw some conclusions from this comparison. 

Initially we will use this data set for learning the SVM model to predict the grade based on the text.
Let¬¥s go randomly split the available data in two sets: a part for training and another (separate set) for testing.
Then, we created the model with the training set and tested it on the testing set.

```{r}
dataSetTest <- dataSet[which(is.na(dataSet$class)), ]       # Save lines with NA's so we can predict its star in the end

dataSet <- dataSet[-which(is.na(dataSet$class)), ]          # Remove lines with NA's

#SVM model

## Splitting the data into a training and test set 
sp <- sample(1: nrow(dataSet), as.integer(0.7*nrow(dataSet)))
tr <- dataSet[sp, ]
ts <- dataSet[-sp, ]

model <- svm(class ~ ., tr)             # obtain the model with the training set
preds <- predict(model, ts)             # apply it to obtain predictions for the test set
preds <- round(preds)

## Calculate the error rate of the predictions 
cm <- table(preds, ts$class)            # confusion matrix
(err <- 1- sum(diag(cm))/sum(cm))
```

Note that the values obtained by the models are in decimal values, but we want our forecast to be an integer value (the number of stars), so we will round, before we calculate the percentage of errors in our forecast, the values obtained using the ``round()`` function.

Now we will try the following models: SVMs (linear and radial kernel), Tree-based Models and MARS.

To compare these models in order to select the best(s) we will take advantage of the functionalities of the package `performanceEstimation`, where in the estimation task we use the mean absolute error (mae) and the mean squared error (mse) as evaluation metric and we use the cross-validation (CV) method.

```{r, fig.align="center"}
# Trying prediction models
exp <- performanceEstimation(
  PredTask(class ~ ., dataSet),
  c( Workflow(learner="earth"),
     Workflow(learner="rpartXse"),
    workflowVariants(learner="svm", learner.pars=list(kernel=c("linear", "radial")))
  ),
  EstimationTask(metrics=c("mae", "mse"), method=CV())
)

# Some results
summary(exp)

plot(exp)

topPerformers(exp)

rankWorkflows(exp,2)
```

We can easily see that the methods with best performing are the SVM with radial kernel for the evaluation metric mean absolute error and Tree-based model for mean squared error.

Having said this, we will predict (using these two methods) the ratings of previously saved reviews (reviews which users did not assign stars).

```{r}
##SVM radial kernel
modelSVM2 <- svm(class ~ ., dataSet, kernel = "radial")
(predSVM2 <- predict(modelSVM2, dataSetTest))

##Tree-based
modelTB <- rpartXse(class ~ ., dataSet)
(predTB <- predict(modelTB, dataSetTest))
```

We can easily verify that tree-based model predicted for all reviews without evaluation, an grade of 5.612981 (ie a 5-star assignment if we round by default).
But we should note that this model has an average absolute deviation between the predictions and true values of 3.014062 which means that this 5-star rating may actually be approximately 2 stars or 8 stars. As well as the results obtained by the best model (SVM), which has an average absolute deviation between the predictions and true values of 2.752745.

We can then conclude that even the best error values (MAE of 2.752745 and MSE of 10.91337) obtained here are still quite relevant for this problem itself, given that the scale we have is 10 (stars) and an average absolute deviation of almost 3 units is quite relevant for the consideration of a positive opinion, an average opinion and a negative opinion.

###Summarise the reviews of a movie

The set we built for learning can and will be used in this section, but without removing the sparse items.

```{r}
dtm <- DocumentTermMatrix(corp)
```


We can start by doing some basic frequency analysis.

```{r, eval=FALSE}
# Find the most frequent words in each review with the following:
findMostFreqTerms(dtm)
```

```{r}
# Find the words that occur more than 50 times
findFreqTerms(dtm, 50)
```

If we analyse the words that occur more than 50 times, we can see that "arnett" is one of the words that users used in their reviews frequently. Since one of the voices in this movie that we are analysing is from actor Will Arnett, we can infer that users talk about what they think his performance was like in this specific movie. We can also see words like "interesting", "adventure", "creative", which are words that are usually used in positive comments, which is in syntony with the movie's score on IMDb (7.8/10 stars).

We can also be interested in finding correlations between words.

```{r}
# Words with a correlation higher than 0.3 with the word "adventure":
findAssocs(dtm, "adventure", 0.3)

# Words with a correlation higher than 0.5 with the word "masterpiece":
findAssocs(dtm, "masterpiece", 0.5)
```

We can now find wordclouds, which are a graphical represantition of the frequencies of terms in a corpus.

```{r, fig.align="center"}
# Simple wordcloud with the 200 most frequent words
wordcloud(corp, colors = rainbow(20), max.words = 200)
```

As we can see, this is not that interesting, since words like "the" and "and" are really frequent and are words that do not describe the movie.

We need to remove what are called **stopwords**.

```{r, fig.align="center"}
corp <- tm_map(corp, removeWords, stopwords("english"))
wordcloud(corp, colors = rainbow(20), max.words = 200)
```

Yet, we can see words like animated and animation, which are words that can be used to describe the same thing. We could stem the words, keeping only the "root" of each word, but we will not do that as the results produced do not seem to be that different from what was previously done.

####Positive and negative words in reviews

What we do now is generate two wordclouds with positive and negative words in reviews. For this, we need the package *qdap*, which requires Java to be up to date.

**NOTE**: Mac OS X users might have to run the following line in a terminal before loading library qdap:

> sudo ln -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib

as seen in this Stack Overflow [thread](http://stackoverflow.com/questions/30738974/rjava-load-error-in-rstudio-r-after-upgrading-to-osx-yosemite), as well as in this [source](https://github.com/snowflakedb/dplyr-snowflakedb/wiki/Configuring-R-rJava-RJDBC-on-Mac-OS-X). 

```{r}
# Calculate the polarity from qdap dictionary
pol <- polarity(corp$content)

# Positive words:
p <- pol$all[,4]

# Negative words:
n <- pol$all[,5]

# Positive words list
positive_words <- unique(setdiff(unlist(p),"-"))
# Negative words list
negative_words <- unique(setdiff(unlist(n),"-"))
```

We can now generate the wordcloud only with positive words.

```{r, fig.align="center"}
pos.tdm <- dtm[,which(colnames(dtm) %in% positive_words)]
m <- as.matrix(pos.tdm)
v <- sort(colSums(m), decreasing = TRUE)
wordcloud(names(v), v, max.words=100,colors=brewer.pal(8, "Dark2"))
title(sub = "Positive Words - Wordcloud")
```

We can also generate the wordcloud only with the negative words.

```{r, fig.align="center"}
neg.tdm <- dtm[,which(colnames(dtm) %in% negative_words) ]
m <- as.matrix(neg.tdm)
v <- sort(colSums(m), decreasing = TRUE)
wordcloud(names(v), v, max.words=100,colors=brewer.pal(8, "Dark2"))         
title(sub = "Negative Words - Wordcloud")
```