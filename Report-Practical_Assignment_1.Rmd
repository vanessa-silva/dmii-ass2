---
title: "Practical Assignment 2 - Web and Text Mining"
author: "João Pires, Vanessa Silva"
date: "30 de Abril de 2017"
output: html_document
---
<br /> 

This report follows the analysis of [IMDB](http://www.imdb.com) movies and their reviews, is the retrieval of relevant information on movies and the analysis of the reviews (and respective scores) assigned by users of the site to the movies.
For this we will use our knowledge regarding web and text mining.

*Web Mining* consists of extracting information from the content of the pages, their links, and users' browsing logs, using data mining tools. 
Thus, we can divide Web Mining into three variants: Web Content Mining; Web Structure Mining; And Web Usage Mining.

*Text Mining* consists of extracting useful information from a collection of documents.
Involves basic pre-processing/text mining operations, such as identification/extraction of representative features, and identification of complex patterns as, e.g. relationships between previously identified concepts.
Text Mining exploits techniques/methodologies from data mining, machine learning, information retrieval, and corpus-based computational linguistics. Where corpus is a collection of documents.


**The Goal**

To use web and text mining to extract and study important informations in the movies and their reviews, of IMDB website.


##Necessary Packages

```{r, eval=FALSE, message=FALSE, warning=FALSE, results='hide'}
#Install the necessary packages
install.packages("rvest")
install.packages("tm")
```


```{r, message=FALSE, warning=FALSE}
#Load the necessary packages
library(rvest)
library(tm)
```


##Tasks

Using the information available in the IMDB site we will accomplish a series of tasks below.


###Find basic information

First we will find basic information as web page, diretor, cast, etc. of a movie based on a query string of the title.

```{r}
#Store web url
url_IMBD <- "http://www.imdb.com/"

# Submit the form on imdb.com for a movie description
Searchform <- function(session, query, type) {
  form <- html_form(session)[[1]]
  form <- set_values(form, q = query, s = type)
 
  return(submit_form(session, form))
}

# Returns web pages link
webPages <- function(listResults) {
  
  results <- listResults[[2]]
  
  #get web page link
  movieLink <- (results %>% html_nodes(".result_text > a") %>% html_attr("href"))
  movieLink <- paste(url_IMBD, movieLink, sep='')
  
  return(movieLink)
  
}

# Returns a list of the all movies that match to the search string in the site
listMoviesAll <- function(query, type) {
  #Simulate a session in an html browser
  sessionMovies <- html_session(url_IMBD)
  
  results <- Searchform(sessionMovies, query, type)
  
  return(list(results %>% html_nodes(".result_text") %>% html_text(), results))
}

# Returns a list of the movies that match to the search string, by decreasing order of confidence
listMovies <- function(query, maxNum, type) {
  
  r <- listMoviesAll(query, type)
  
  TMlist <- r[[1]]
  
  movieLink <- webPages(r)
  
  #get rankings
  movieRanking <- lapply(movieLink, . %>% read_html() %>% html_nodes("strong span") %>% html_text())
  movieRanking <- as.array(movieRanking)
  
  #get date frame with movies and their ranking
  ResulList <- data.frame(as.array(TMlist),movieRanking)
  names(ResulList) <- c("Movies", "Ranking")
  ResulList$Ranking <- as.double(ResulList$Ranking)
  
  return(as.list((head(ResulList[order(ResulList$Ranking, decreasing=TRUE), ], maxNum))$Movies))
}

Type <- "tt"
Query <- "Lego"       #search string
#add an extra parameter that controls the maximum number of retrieved movies
NumMovies <- 10       #top 10

ResultListMovies <- listMovies(Query, NumMovies, Type)    #Demora um bom bocado a executar

##For each movie the basic information is returned as a list: 

# Get link from web pages of n movies

FullList <- listMoviesAll(Query, Type)
results <- FullList[2][[1]]
movieLink <- webPages(list(results %>% html_nodes(".result_text") %>% html_text(), results))

links <- matrix(nrow=10,ncol=2)
idx <- 1

for(i in 1:length(movieLink))
  for(j in 1:length(ResultListMovies))
    if (ResultListMovies[j][[1]] == FullList[1][[1]][i]){
      links[idx, 1] <- movieLink[i]
      links[idx,2] <- j
      idx <- idx+1
    }

movieLink <- matrix(ncol = 10)

for(i in 1:10) {
  idx <- as.integer(links[i,2])
  movieLink[idx] <- links[i,1]
}
movieLink <- as.character(movieLink)

# Find basic information

listBasInf <- list()

for(i in 1:NumMovies) {
  l <- list()
  j <- 1
  
  ##web page
  l[[j]] <- list(WebPage = (movieLink[i] %>% read_html()))
  j <- j+1
  ##cast
  l[[j]] <- list(Cast = (movieLink[i] %>% read_html() %>% html_nodes("#titleCast .itemprop span") %>% html_text()))
  j <- j+1
  ##description
  l[[j]] <- list(Description = (movieLink[i] %>% read_html() %>% html_nodes(".summary_text") %>% html_text()))
  j <- j+1
  ##storyline
  l[[j]] <- list(Storyline = (movieLink[i] %>% read_html() %>% html_nodes("#titleStoryLine p") %>% html_text()))
  j <- j+1
  ##original title
  l[[j]] <- list(OriginalTitle = (movieLink[i] %>% read_html() %>% html_nodes(".originalTitle") %>% html_text()))
  j <- j+1
  ##runtime
  l[[j]] <- list(Runtime = (movieLink[i] %>% read_html() %>% html_nodes("#titleDetails time") %>% html_text())) 
  j <- j+1
  ##genre
  l[[j]] <- list(Genre = (movieLink[i] %>% read_html() %>% html_nodes(".subtext .itemprop") %>% html_text()))
  j <- j+1
  ##date
  l[[j]] <- list(Date = (movieLink[i] %>% read_html() %>% html_nodes(".subtext a~ .ghost+ a") %>% html_text()))
  j <- j+1
  ##rating
  l[[j]] <- list(Ratink = (movieLink[i] %>% read_html() %>% html_nodes("strong span") %>% html_text()))
  j <- j+1
  ##metascore
  l[[j]] <- list(Metascore = movieLink[i] %>% read_html() %>% html_nodes(".score_favorable span") %>% html_text())
  
  listBasInf[[i]] <- list(l)
}
```



###Obtain the information on all reviews from a movie

We now present a set of functions to perform the task of finding all reviews for a movie, as well as the rating reviews given by the review writer.

One can read, in the page [User Reviews Guidelines](http://www.imdb.com/help/show_leaf?commentsguidelines) the following: "The minimum length for reviews is 5 lines of text."

With this we can see that the text field is mandatory, but the rating field is not. With that in mind, our function will return `r NA` whenever the user did not provide any rating. The rating provided is then an integer between 1 and 10, the scale given by IMDb for ratings.

```{r}
# Function to return the comments
commentOfReview <- function (linkOfMovie) {
  movie <- linkOfMovie %>% read_html()
  
  # Number of comments
  n_reviews <- as.integer(strsplit((movie %>% html_nodes(".user-comments a"))[4] %>% html_text(), " ")[[1]][3])
  
  # Link of the page with all reviews
  reviews_page_link <- paste(linkOfMovie, "reviews?count=", n_reviews, "&start=0", sep = "")
  
  comments <- reviews_page_link %>% read_html() %>% html_nodes("#pagecontent") %>% html_nodes("div+ p") %>% html_text()
  
  return(comments)
}

# Function to return rating of a review
ratingOfReview <- function(linkOfMovie) {
  movie <- linkOfMovie %>% read_html()
  
  # Number of comments
  n_reviews <- as.integer(strsplit((movie %>% html_nodes(".user-comments a"))[4] %>% html_text(), " ")[[1]][3])
  
  # Link of the page with all reviews
  reviews_page_link <- paste(linkOfMovie, "reviews?count=", n_reviews, "&start=0", sep = "")
  
  reviews_images <- reviews_page_link %>% read_html() %>% html_nodes("#tn15content") %>% html_nodes("div img")
  
  ans <- NULL
  cnt <- 1
  for(i in 1:length(reviews_images)) {
    if(!is.na(reviews_images[i] %>% html_attr("class")) && reviews_images[i] %>% html_attr("class") == "avatar") {
      if(i == length(reviews_images)) {
        ans[cnt] <- NA
        cnt <- cnt + 1
      }
      else {
        if(!is.na(reviews_images[i + 1] %>% html_attr("class")) && reviews_images[i + 1] %>% html_attr("class") == "avatar") {
          ans[cnt] <- NA
          cnt <- cnt + 1
        }
      }
    }
    else {
      ans[cnt] <- as.integer(substr(reviews_images[i] %>% html_attr("alt"), 1, 1))
      cnt <- cnt + 1
    }
  }
  
  return(ans)
}

# Function to return the comments and stars given in review
fullReviews <- function(linkOfMovie) {
  ans <- data.frame(comment = commentOfReview(linkOfMovie), rating = ratingOfReview(linkOfMovie), stringsAsFactors = FALSE)
  
  return(ans)
}
```

We can try now this with the Lego Movie, as an example:

```{r, eval=FALSE}
# Lego movie:
lego_movie_link <- "http://www.imdb.com/title/tt1490017/"
dat <- fullReviews(lego_movie_link)
```

###Model to predict the grade

####Build a data set for learning

```{r, eval=FALSE}
#First, let's build a corpus of directories with the reviews of the movie "Lego" so that we can perform the following preprocessing tasks.
corp <- Corpus(VectorSource(dat$comment))

#The next thing to do is preprocessing the corpus: Clean-ups:
corp <-  tm_map(corp, stripWhitespace)                      #Strip extra whitespace
corp <-  tm_map(corp, content_transformer(tolower))         #Turn everything to lowercase
#corp <-  tm_map(corp, removeWords, stopwords("the", "that", ...?))    #Remove stopwords
corp <-  tm_map(corp, removePunctuation)                    #Remove punctuation
##Neste caso concreto acho que vale a pensa manter os numero? pelo intuito de que no texto pode apresentar uma classificação numerica e nao associa-la no campo das stars ?
#corp <-  tm_map(corp, removeNumbers)                        #Remove numbers

dtm <- DocumentTermMatrix(corp)
dtm <- removeSparseTerms(dtm, 0.6)                        #Remove sparse terms

#Data set for learning
dataSet <- cbind(data.frame(as.matrix(dtm), class = dat$rating))
```

####Try a few prediction models and draw conclusions

###Summarise the reviews of a movie

The set we built for learning can and will be used in this section, but without removing the sparse items.

```{r}
dtm <- DocumentTermMatrix(corp)
```


We can start by doing some basic frequency analysis.

```{r, eval=FALSE}
# Find the most frequent words in each review with the following:
findMostFreqTerms(dtm)
```

```{r}
# Find the words that occur more than 50 times
findFreqTerms(dtm, 50)
```

If we analyse the words that occur more than 50 times, we can see that "arnett" is one of the words that users used in their reviews frequently. Since one of the voices in this movie that we are analysing is from actor Will Arnett, we can infer that users talk about what they think his performance was like in this specific movie. We can also see words like "interesting", "adventure", "creative", which are words that are usually used in positive comments, which is in syntony with the movie's score on IMDb (7.8/10 stars).

We can also be interested in finding correlations between words.

```{r}
# Words with a correlation higher than 0.3 with the word "adventure":
findAssocs(dtm, "adventure", 0.3)

# Words with a correlation higher than 0.5 with the word "masterpiece":
findAssocs(dtm, "masterpiece", 0.5)
```

We can now find wordclouds, which are a graphical represantition of the frequencies of terms in a corpus.

```{r}
# Simple wordcloud with the 200 most frequent words
wordcloud(corp, colors = rainbow(20), max.words = 200)
```

As we can see, this is not that interesting, since words like "the" and "and" are really frequent and are words that do not describe the movie.

We need to remove what are called **stopwords**.

```{r}
corp <- tm_map(corp, removeWords, stopwords("english"))
wordcloud(corp, colors = rainbow(20), max.words = 200)
```

Yet, we can see words like animated and animation, which are words that can be used to describe the same thing. We could stem the words, keeping only the "root" of each word, but we will not do that as the results produced do not seem to be that different from what was previously done.

####Positive and negative words in reviews

What we do now is generate two wordclouds with positive and negative words in reviews. For this, we need the package *qdap*, which requires Java to be up to date.

**NOTE**: Mac OS X users might have to run the following line in a terminal before loading library qdap:

> sudo ln -s $(/usr/libexec/java_home)/jre/lib/server/libjvm.dylib /usr/local/lib

as seen in this Stack Overflow [thread](http://stackoverflow.com/questions/30738974/rjava-load-error-in-rstudio-r-after-upgrading-to-osx-yosemite), as well as in this [source](https://github.com/snowflakedb/dplyr-snowflakedb/wiki/Configuring-R-rJava-RJDBC-on-Mac-OS-X). 

```{r}
# Calculate the polarity from qdap dictionary
pol <- polarity(corp$content)

# Positive words:
p <- pol$all[,4]

# Negative words:
n <- pol$all[,5]

# Positive words list
positive_words <- unique(setdiff(unlist(p),"-"))
# Negative words list
negative_words <- unique(setdiff(unlist(n),"-"))
```

We can now generate the wordcloud only with positive words.

```{r, fig.align="center"}
pos.tdm <- dtm[,which(colnames(dtm) %in% positive_words)]
m <- as.matrix(pos.tdm)
v <- sort(colSums(m), decreasing = TRUE)
wordcloud(names(v), v, max.words=100,colors=brewer.pal(8, "Dark2"))
title(sub = "Positive Words - Wordcloud")
```

We can also generate the wordcloud only with the negative words.

```{r, fig.align="center"}
neg.tdm <- dtm[,which(colnames(dtm) %in% negative_words) ]
m <- as.matrix(neg.tdm)
v <- sort(colSums(m), decreasing = TRUE)
wordcloud(names(v), v, max.words=100,colors=brewer.pal(8, "Dark2"))         
title(sub = "Negative Words - Wordcloud")
```